## Luozhen计划

目前想到以下两点

1. 特征

   本来以为分本分类的特征就是分词之后所形成的词向量, 但现在觉得可能有点狭隘了, 如果仅仅以分词或做了简单处理后作为特征, 有点太瞎折腾了......

   * 深入英文分词, 参见["英文分词_luozhen"](./英文分词_luozhen) 这篇, 探索一下. 当然可以先一开始做出个大致的东西, 日后再优化~
   * 想到可以用模型选择特征`特征选择 特征提取` , 因为文本的特征可能对分类有明显的偏好, 这里所说的偏好, 个人是指某个词的出现对分类具有很大影响, 或相关性. 虽然0820meeting说词貌似与分类不存在强相关.....(暂定)
   * 特征提取, 试试机器学习算法将本文向量转化为等长的向量(使用且不限于上次使用的word2vec)

2. 模型

   模型在0820meeting上, 注意到老周你说xgboost效果好, 个人也想试试, 但是更想试试CNN, 因为恰好自己现在工作上接触到文本的CNN, 所以想试试

   * CNN/DNN的尝试

   * xgboost(暂定)

---

总而言之, 现在的想法是大致能搞出个模型, 在之后再根据他人或自己的分析, 尝试其他方面. 